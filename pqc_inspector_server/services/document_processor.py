# File: pqc_inspector_server/services/document_processor.py
# ğŸ“„ PDF, DOCX, TXT ë“± ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

import os
import re
from typing import List, Dict, Any, Optional
from pathlib import Path
import hashlib

# PDF ì²˜ë¦¬ìš©
try:
    import fitz  # PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# DOCX ì²˜ë¦¬ìš©
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

# ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ìš©
try:
    import markdown
    from bs4 import BeautifulSoup
    MARKDOWN_AVAILABLE = True
except ImportError:
    MARKDOWN_AVAILABLE = False

class DocumentProcessor:
    def __init__(self):
        self.supported_formats = []
        self.chunk_size = 1000  # ê¸°ë³¸ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        self.chunk_overlap = 100  # ì²­í¬ ê°„ ì¤‘ë³µ ë¬¸ì ìˆ˜

        # ì§€ì› ê°€ëŠ¥í•œ í˜•ì‹ í™•ì¸
        if PDF_AVAILABLE:
            self.supported_formats.extend(['.pdf'])
        if DOCX_AVAILABLE:
            self.supported_formats.extend(['.docx'])
        if MARKDOWN_AVAILABLE:
            self.supported_formats.extend(['.md', '.markdown'])

        self.supported_formats.extend(['.txt', '.py', '.c', '.cpp', '.h', '.js', '.java'])

        print(f"DocumentProcessor ì´ˆê¸°í™”ë¨. ì§€ì› í˜•ì‹: {self.supported_formats}")

    def is_supported(self, file_path: str) -> bool:
        """íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        suffix = Path(file_path).suffix.lower()
        return suffix in self.supported_formats

    async def process_document(self, file_path: str, agent_type: str = "auto") -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì¶”ê°€ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        if not self.is_supported(file_path):
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {Path(file_path).suffix}")

        print(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")

        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        file_info = self._extract_file_info(file_path)

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw_text = self._extract_text(file_path)

        # ì „ì²˜ë¦¬
        processed_text = self._preprocess_text(raw_text)

        # ì²­í‚¹
        chunks = self._chunk_text(processed_text)

        # ì—ì´ì „íŠ¸ íƒ€ì… ìë™ ê²°ì •
        if agent_type == "auto":
            agent_type = self._determine_agent_type(file_path, processed_text)

        # ë©”íƒ€ë°ì´í„° ìƒì„±
        result = {
            "file_info": file_info,
            "agent_type": agent_type,
            "chunks": chunks,
            "total_chunks": len(chunks),
            "original_text_length": len(raw_text),
            "processed_text_length": len(processed_text)
        }

        print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return result

    def _extract_file_info(self, file_path: str) -> Dict[str, Any]:
        """íŒŒì¼ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        path_obj = Path(file_path)
        stat = path_obj.stat()

        with open(file_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()

        return {
            "filename": path_obj.name,
            "file_size": stat.st_size,
            "file_extension": path_obj.suffix.lower(),
            "file_hash": file_hash,
            "creation_time": stat.st_ctime,
            "modification_time": stat.st_mtime
        }

    def _extract_text(self, file_path: str) -> str:
        """íŒŒì¼ í˜•ì‹ì— ë”°ë¼ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        suffix = Path(file_path).suffix.lower()

        if suffix == '.pdf':
            return self._extract_pdf_text(file_path)
        elif suffix == '.docx':
            return self._extract_docx_text(file_path)
        elif suffix in ['.md', '.markdown']:
            return self._extract_markdown_text(file_path)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼
            return self._extract_plain_text(file_path)

    def _extract_pdf_text(self, file_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not PDF_AVAILABLE:
            raise ImportError("PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ PyMuPDFë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install PyMuPDF")

        text = ""
        try:
            doc = fitz.open(file_path)
            for page_num in range(len(doc)):
                page = doc[page_num]
                page_text = page.get_text()

                # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n"
                text += page_text

            doc.close()
            print(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(doc)} í˜ì´ì§€")
            return text

        except Exception as e:
            raise Exception(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def _extract_docx_text(self, file_path: str) -> str:
        """DOCXì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not DOCX_AVAILABLE:
            raise ImportError("DOCX ì²˜ë¦¬ë¥¼ ìœ„í•´ python-docxë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install python-docx")

        try:
            doc = Document(file_path)
            text = ""

            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"

            print(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(doc.paragraphs)} ë¬¸ë‹¨")
            return text

        except Exception as e:
            raise Exception(f"DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def _extract_markdown_text(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if not MARKDOWN_AVAILABLE:
            raise ImportError("ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ë¥¼ ìœ„í•´ markdownê³¼ beautifulsoup4ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                md_content = f.read()

            # ë§ˆí¬ë‹¤ìš´ì„ HTMLë¡œ ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            html = markdown.markdown(md_content)
            soup = BeautifulSoup(html, 'html.parser')
            text = soup.get_text()

            print(f"Markdown í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
            return text

        except Exception as e:
            raise Exception(f"Markdown í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def _extract_plain_text(self, file_path: str) -> str:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            encodings = ['utf-8', 'utf-16', 'cp949', 'euc-kr', 'latin-1']

            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        text = f.read()
                    print(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì™„ë£Œ (ì¸ì½”ë”©: {encoding})")
                    return text
                except UnicodeDecodeError:
                    continue

            raise Exception("ì§€ì›í•˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        except Exception as e:
            raise Exception(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")

    def _preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        # ê³¼ë„í•œ ê³µë°± ì œê±°
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•„ìš”ì‹œ)
        text = text.strip()

        return text

    def _chunk_text(self, text: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        chunks = []

        # ë¬¸ë‹¨ ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ë¶„í• 
        paragraphs = text.split('\n\n')

        current_chunk = ""
        chunk_index = 0

        for paragraph in paragraphs:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
            test_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph

            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì €ì¥
                if current_chunk:
                    chunks.append(self._create_chunk(current_chunk, chunk_index))
                    chunk_index += 1

                # ë‹¨ì¼ ë¬¸ë‹¨ì´ ë„ˆë¬´ í´ ê²½ìš° ê°•ì œ ë¶„í• 
                if len(paragraph) > self.chunk_size:
                    sub_chunks = self._force_split_text(paragraph, chunk_index)
                    chunks.extend(sub_chunks)
                    chunk_index += len(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = paragraph

        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk:
            chunks.append(self._create_chunk(current_chunk, chunk_index))

        return chunks

    def _create_chunk(self, text: str, index: int) -> Dict[str, Any]:
        """ì²­í¬ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            "index": index,
            "content": text.strip(),
            "char_count": len(text),
            "word_count": len(text.split()),
            "chunk_hash": hashlib.md5(text.encode()).hexdigest()[:8]
        }

    def _force_split_text(self, text: str, start_index: int) -> List[Dict[str, Any]]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°•ì œë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        chunks = []
        index = start_index

        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk_text = text[i:i + self.chunk_size]
            chunks.append(self._create_chunk(chunk_text, index))
            index += 1

        return chunks

    def _determine_agent_type(self, file_path: str, text: str) -> str:
        """íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ì—ì´ì „íŠ¸ íƒ€ì…ì„ ê²°ì •í•©ë‹ˆë‹¤."""
        path_obj = Path(file_path)
        filename = path_obj.name.lower()
        text_lower = text.lower()

        # 1. ë””ë ‰í† ë¦¬ êµ¬ì¡° ê¸°ë°˜ ìš°ì„  ë¶„ë¥˜
        # data/documents/source_code/ í˜•íƒœì˜ ê²½ë¡œì—ì„œ ì—ì´ì „íŠ¸ íƒ€ì… ì¶”ì¶œ
        path_parts = path_obj.parts
        valid_agents = ['source_code', 'binary', 'log_conf']

        for part in path_parts:
            if part in valid_agents:
                print(f"ğŸ“ ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë¶„ë¥˜: {file_path} â†’ {part}")
                return part

        # 2. íŒŒì¼ëª… ê¸°ë°˜ ë¶„ë¥˜
        if any(keyword in filename for keyword in ['log', 'audit', 'config', 'conf', 'setting']):
            return 'log_conf'
        elif any(keyword in filename for keyword in ['binary', 'asm', 'disasm']):
            return 'binary'

        # 3. ë‚´ìš© ê¸°ë°˜ ë¶„ë¥˜
        source_code_keywords = ['function', 'class', 'import', 'include', 'def ', 'public class']
        binary_keywords = ['assembly', 'disassembly', 'binary analysis', 'executable']
        log_keywords = ['log', 'syslog', 'audit', 'connection', 'authentication', 'configuration', 'config', 'json', 'yaml', 'xml', 'settings']

        keyword_scores = {
            'source_code': sum(1 for kw in source_code_keywords if kw in text_lower),
            'binary': sum(1 for kw in binary_keywords if kw in text_lower),
            'log_conf': sum(1 for kw in log_keywords if kw in text_lower)
        }

        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íƒ€ì… ì„ íƒ
        best_type = max(keyword_scores.items(), key=lambda x: x[1])

        if best_type[1] > 0:
            print(f"ğŸ“ ë‚´ìš© ê¸°ë°˜ ë¶„ë¥˜: {filename} â†’ {best_type[0]} (ì ìˆ˜: {best_type[1]})")
            return best_type[0]
        else:
            # ê¸°ë³¸ê°’: source_code
            print(f"ğŸ”§ ê¸°ë³¸ê°’ ë¶„ë¥˜: {filename} â†’ source_code")
            return 'source_code'

    def get_document_summary(self, processed_doc: Dict[str, Any]) -> str:
        """ë¬¸ì„œ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        info = processed_doc["file_info"]
        summary = f"""
ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ìš”ì•½
- íŒŒì¼ëª…: {info['filename']}
- í¬ê¸°: {info['file_size']:,} bytes
- í˜•ì‹: {info['file_extension']}
- ì—ì´ì „íŠ¸ íƒ€ì…: {processed_doc['agent_type']}
- ì´ ì²­í¬ ìˆ˜: {processed_doc['total_chunks']}
- ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {processed_doc['original_text_length']:,} ë¬¸ì
- ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {processed_doc['processed_text_length']:,} ë¬¸ì
"""
        return summary.strip()

# ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ í•¨ìˆ˜
def get_document_processor():
    return DocumentProcessor()