# File: pqc_inspector_server/services/ai_service.py
# ğŸ¤– ìƒìš© AI API (GPT-4.1, Gemini 2.5 Flash)ì™€ í†µì‹ í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

import httpx
import json
from typing import Dict, Any, Optional
from ..core.config import settings

class AIService:
    def __init__(self):
        self.openai_api_key = settings.OPENAI_API_KEY
        self.google_api_key = settings.GOOGLE_API_KEY
        self.openai_base_url = "https://api.openai.com/v1"
        self.google_base_url = "https://generativelanguage.googleapis.com/v1beta"
        self.ollama_base_url = settings.OLLAMA_BASE_URL
        print("AIServiceê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    async def generate_response(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        ìƒìš© AI ëª¨ë¸ì—ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.
        """
        try:
            print(f"ğŸ¤– AI ëª¨ë¸ í˜¸ì¶œ ì‹œì‘: {model}")
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} characters")

            import time
            start_time = time.time()

            if model.startswith("gpt-"):
                response = await self._call_openai(model, prompt, system_prompt)
            elif model.startswith("gemini-"):
                response = await self._call_google(model, prompt, system_prompt)
            elif ":" in model:  # Ollama ëª¨ë¸ (ì˜ˆ: llama3:8b)
                response = await self._call_ollama(model, prompt, system_prompt)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model}")

            end_time = time.time()
            duration = end_time - start_time

            print(f"âœ… AI ì‘ë‹µ ì™„ë£Œ: {duration:.2f}ì´ˆ")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response.get('content', ''))} characters")

            response["actual_duration"] = duration
            return response

        except Exception as e:
            print(f"âŒ AI ëª¨ë¸ '{model}' í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": None
            }

    async def _call_openai(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """OpenAI API í˜¸ì¶œ"""
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.openai_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openai_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "temperature": 0.1
                },
                timeout=60.0
            )

            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "content": data["choices"][0]["message"]["content"],
                    "model": model,
                    "usage": data.get("usage", {})
                }
            else:
                return {
                    "success": False,
                    "error": f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}",
                    "content": None
                }

    async def _call_google(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """Google Gemini API í˜¸ì¶œ"""
        # Gemini APIëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.google_base_url}/models/{model}:generateContent",
                headers={
                    "Content-Type": "application/json"
                },
                params={"key": self.google_api_key},
                json={
                    "contents": [{
                        "parts": [{"text": full_prompt}]
                    }],
                    "generationConfig": {
                        "temperature": 0.1,
                        "maxOutputTokens": 4096
                    }
                },
                timeout=60.0
            )

            if response.status_code == 200:
                data = response.json()
                if "candidates" in data and len(data["candidates"]) > 0:
                    content = data["candidates"][0]["content"]["parts"][0]["text"]
                    return {
                        "success": True,
                        "content": content,
                        "model": model,
                        "usage": data.get("usageMetadata", {})
                    }
                else:
                    return {
                        "success": False,
                        "error": "Google APIì—ì„œ ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                        "content": None
                    }
            else:
                return {
                    "success": False,
                    "error": f"Google API ì˜¤ë¥˜: {response.status_code} - {response.text}",
                    "content": None
                }

    async def _call_ollama(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """Ollama API í˜¸ì¶œ"""
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": prompt})

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.ollama_base_url}/api/chat",
                headers={
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.1
                    }
                },
                timeout=120.0
            )

            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "content": data["message"]["content"],
                    "model": model,
                    "usage": {
                        "prompt_tokens": data.get("prompt_eval_count", 0),
                        "completion_tokens": data.get("eval_count", 0),
                        "total_tokens": data.get("prompt_eval_count", 0) + data.get("eval_count", 0)
                    }
                }
            else:
                return {
                    "success": False,
                    "error": f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}",
                    "content": None
                }

# ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ í•¨ìˆ˜
def get_ai_service():
    return AIService()